{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 Preprocessing, Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Introduction**\n",
    "\n",
    "IEEE Computational Intelligence Society (IEEE-CIS) works across a variety of AI and machine learning areas, including deep neural networks, fuzzy systems, evolutionary computation, and swarm intelligence. Today they’re partnering with Vesta, the world’s leading payment service company, seeking the best solutions for the fraud detection industry. The fraud prevention system used by Vesta is actually saving consumers millions of dollars per year. Researchers from the IEEE-CIS want to improve fraud detection accuracy but also the customer experiences.\n",
    "\n",
    "**Data Source**\n",
    "\n",
    "The data comes from Vesta’s real-world e-commerce transactions and contains a wide range of features from device type to product features, available in Kaggle competition (https://www.kaggle.com/c/ieee-fraud-detection/data). Only train_identity and train_transaction datasets will be used for this project.\n",
    "\n",
    "**The Data Science Method**  \n",
    "\n",
    "1.   Problem Identification \n",
    "\n",
    "2.   Data Wrangling \n",
    " \n",
    "3.   Exploratory Data Analysis\n",
    "\n",
    "4.   **Pre-processing and Training Data Development**\n",
    "    - Create new features\n",
    "    - Standardize numeric features\n",
    "    - Split into testing and training datasets\n",
    "    - Resampling training dataset\n",
    "\n",
    "5.   Modeling \n",
    "\n",
    "6.   Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TransactionID  isFraud  TransactionDT  TransactionAmt ProductCD  card1  \\\n",
      "0        2987001        0          86401            29.0         W   2755   \n",
      "1        2987002        0          86469            59.0         W   4663   \n",
      "2        2987003        0          86499            50.0         W  18132   \n",
      "3        2987005        0          86510            49.0         W   5937   \n",
      "4        2987006        0          86522           159.0         W  12308   \n",
      "\n",
      "   card2  card3       card4  card5  ... V281  V282  V283 V284  V286  V291  \\\n",
      "0  404.0  150.0  mastercard  102.0  ...  0.0   1.0   1.0  0.0   0.0   1.0   \n",
      "1  490.0  150.0        visa  166.0  ...  0.0   1.0   1.0  0.0   0.0   1.0   \n",
      "2  567.0  150.0  mastercard  117.0  ...  0.0   0.0   0.0  0.0   0.0   1.0   \n",
      "3  555.0  150.0        visa  226.0  ...  0.0   1.0   1.0  0.0   0.0   1.0   \n",
      "4  360.0  150.0        visa  166.0  ...  0.0   1.0   1.0  0.0   0.0   1.0   \n",
      "\n",
      "   V297  V299  V305 V311  \n",
      "0   0.0   0.0   1.0  0.0  \n",
      "1   0.0   0.0   1.0  0.0  \n",
      "2   0.0   0.0   1.0  0.0  \n",
      "3   0.0   0.0   1.0  0.0  \n",
      "4   0.0   0.0   1.0  0.0  \n",
      "\n",
      "[5 rows x 60 columns]\n"
     ]
    }
   ],
   "source": [
    "path=\".../data\"\n",
    "os.chdir(path) \n",
    "\n",
    "df = pd.read_csv('step3_output.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'P_emaildomain', 'C3', 'D1', 'D4', 'D10', 'D15', 'M6', 'V25', 'V26', 'V46', 'V47', 'V55', 'V56', 'V61', 'V62', 'V66', 'V67', 'V77', 'V78', 'V82', 'V83', 'V98', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V114', 'V115', 'V116', 'V118', 'V120', 'V121', 'V122', 'V124', 'V281', 'V282', 'V283', 'V284', 'V286', 'V291', 'V297', 'V299', 'V305', 'V311']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TransactionID  TransactionDT  TransactionAmt ProductCD  card1  card2  \\\n",
      "0        3663549       18403224           31.95         W  10409  111.0   \n",
      "1        3663550       18403263           49.00         W   4272  111.0   \n",
      "2        3663551       18403310          171.00         W   4476  574.0   \n",
      "3        3663552       18403310          284.95         W  10989  360.0   \n",
      "4        3663553       18403317           67.95         W  18018  452.0   \n",
      "\n",
      "   card3       card4  card5  card6  ...  V281  V282 V283  V284  V286  V291  \\\n",
      "0  150.0        visa  226.0  debit  ...   0.0   0.0  0.0   0.0   0.0   1.0   \n",
      "1  150.0        visa  226.0  debit  ...   0.0   0.0  0.0   0.0   0.0   1.0   \n",
      "2  150.0        visa  226.0  debit  ...   0.0   0.0  0.0   0.0   0.0   2.0   \n",
      "3  150.0        visa  166.0  debit  ...   0.0   1.0  1.0   0.0   0.0   1.0   \n",
      "4  150.0  mastercard  117.0  debit  ...   0.0   1.0  1.0   1.0   1.0   1.0   \n",
      "\n",
      "   V297  V299 V305       V311  \n",
      "0   0.0   0.0  1.0   0.000000  \n",
      "1   0.0   0.0  1.0   0.000000  \n",
      "2   0.0   0.0  1.0   0.000000  \n",
      "3   0.0   0.0  1.0   0.000000  \n",
      "4   0.0   0.0  1.0  67.949997  \n",
      "\n",
      "[5 rows x 59 columns]\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test = test[['TransactionID', 'TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'P_emaildomain', 'C3', 'D1', 'D4', 'D10', 'D15', 'M6', 'V25', 'V26', 'V46', 'V47', 'V55', 'V56', 'V61', 'V62', 'V66', 'V67', 'V77', 'V78', 'V82', 'V83', 'V98', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V114', 'V115', 'V116', 'V118', 'V120', 'V121', 'V122', 'V124', 'V281', 'V282', 'V283', 'V284', 'V286', 'V291', 'V297', 'V299', 'V305', 'V311']]\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>M6</th>\n",
       "      <td>0.182761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>addr2</th>\n",
       "      <td>0.139652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>addr1</th>\n",
       "      <td>0.139652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card2</th>\n",
       "      <td>0.017010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card5</th>\n",
       "      <td>0.007059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card6</th>\n",
       "      <td>0.003258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card3</th>\n",
       "      <td>0.003258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card4</th>\n",
       "      <td>0.003258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        percent\n",
       "M6     0.182761\n",
       "addr2  0.139652\n",
       "addr1  0.139652\n",
       "card2  0.017010\n",
       "card5  0.007059\n",
       "card6  0.003258\n",
       "card3  0.003258\n",
       "card4  0.003258"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nas = pd.DataFrame(df.isnull().sum().sort_values(ascending=False)/len(df),columns = ['percent'])\n",
    "pos = nas['percent'] > 0\n",
    "nas[pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Feature Engineering\n",
    "- Create new features: \n",
    "    - TransactionAmt_log - log of transaction amount\n",
    "    - Transaction_day - day of the week in which a transaction happened\n",
    "    - Transaction_hour - hour of the day in which a transaction happened\n",
    "- Card features: frequency encoding\n",
    "- P_emaildomain:\n",
    "    - Fill NAs with \"email_not_provided\"\n",
    "    - Split the email domain\n",
    "- Encode objects\n",
    "- V features: normalize against themselves\n",
    "- Fill NAs with -1\n",
    "\n",
    "We will match train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New features\n",
    "df['TransactionAmt_log'] = np.log(df['TransactionAmt'])\n",
    "df['Transaction_day'] = np.floor((df['TransactionDT'] / (3600 * 24) - 1) % 7)\n",
    "df['Transaction_hour'] = np.floor(df['TransactionDT'] / 3600) % 24\n",
    "\n",
    "\n",
    "test['TransactionAmt_log'] = np.log(test['TransactionAmt'])\n",
    "test['Transaction_day'] = np.floor((test['TransactionDT'] / (3600 * 24) - 1) % 7)\n",
    "test['Transaction_hour'] = np.floor(test['TransactionDT'] / 3600) % 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Card features encoding\n",
    "for col in ['card1', 'card2', 'card3', 'card4', 'card5', 'card6']:\n",
    "    freq = df[col].value_counts(dropna=False).to_dict()\n",
    "    df[col+'_freq'] = df[col].map(freq)\n",
    "    \n",
    "for col in ['card1', 'card2', 'card3', 'card4', 'card5', 'card6']:\n",
    "    freq = test[col].value_counts(dropna=False).to_dict()\n",
    "    test[col+'_freq'] = test[col].map(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# email feature\n",
    "df['P_emaildomain'] = df['P_emaildomain'].fillna('email_not_provided')\n",
    "df['P_prefix'] = df['P_emaildomain'].apply(lambda x: x.split('.')[0])\n",
    "\n",
    "test['P_emaildomain'] = test['P_emaildomain'].fillna('email_not_provided')\n",
    "test['P_prefix'] = test['P_emaildomain'].apply(lambda x: x.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objects\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "for col in df.drop('isFraud', axis=1).columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(df[col].astype(str).values))\n",
    "        df[col] = le.transform(list(df[col].astype(str).values))\n",
    "\n",
    "for col in test.columns:\n",
    "    if test[col].dtype == 'object' or 'O':\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(test[col].astype(str).values))\n",
    "        test[col] = le.transform(list(test[col].astype(str).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V features - normalization\n",
    "V = df[['V25', 'V26', 'V46', 'V47', 'V55', 'V56', 'V61', 'V62', 'V66', 'V67', 'V77', 'V78', 'V82', 'V83', 'V98', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V114', 'V115', 'V116', 'V118', 'V120', 'V121', 'V122', 'V124', 'V281', 'V282', 'V283', 'V284', 'V286', 'V291', 'V297', 'V299', 'V305', 'V311']]\n",
    "for v in V:\n",
    "    df[v] = (df[v] - df[v].mean()) / df[v].std()\n",
    "    \n",
    "V_test = test[['V25', 'V26', 'V46', 'V47', 'V55', 'V56', 'V61', 'V62', 'V66', 'V67', 'V77', 'V78', 'V82', 'V83', 'V98', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V114', 'V115', 'V116', 'V118', 'V120', 'V121', 'V122', 'V124', 'V281', 'V282', 'V283', 'V284', 'V286', 'V291', 'V297', 'V299', 'V305', 'V311']]\n",
    "for v in V_test:\n",
    "    test[v] = (test[v] - test[v].mean()) / test[v].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>addr2</th>\n",
       "      <td>0.139652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>addr1</th>\n",
       "      <td>0.139652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card2</th>\n",
       "      <td>0.017010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card5</th>\n",
       "      <td>0.007059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card3</th>\n",
       "      <td>0.003258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        percent\n",
       "addr2  0.139652\n",
       "addr1  0.139652\n",
       "card2  0.017010\n",
       "card5  0.007059\n",
       "card3  0.003258"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nas = pd.DataFrame(df.isnull().sum().sort_values(ascending=False)/len(df),columns = ['percent'])\n",
    "pos = nas['percent'] > 0\n",
    "nas[pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['addr1', 'addr2', 'card2', 'card5', 'card3']:\n",
    "    df[col].fillna(-1, inplace=True)\n",
    "\n",
    "for col in ['addr1', 'addr2', 'card2', 'card5', 'card3']:\n",
    "    test[col].fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dtype('int64'), dtype('int64'), dtype('int64'), dtype('float64'), dtype('int64'), dtype('int64'), dtype('float64'), dtype('float64'), dtype('int64'), dtype('float64'), dtype('int64'), dtype('float64'), dtype('float64'), dtype('int64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('int64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64')]\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['card4', 'card6', 'C3', 'D1', 'D4', 'D10', 'D15', 'M6', 'V25', 'V26', 'V46', 'V47', 'V55', 'V56', 'V61', 'V62', 'V66', 'V67', 'V77', 'V78', 'V82', 'V83', 'V107', 'V281', 'V282', 'V283', 'V284', 'V286', 'V291', 'V297', 'V299', 'V305', 'V311']\n"
     ]
    }
   ],
   "source": [
    "empty = []\n",
    "for col in test:\n",
    "    if test[col].isnull().sum() > 0:\n",
    "        empty.append(col)\n",
    "print(empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in test:\n",
    "    test[col].fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64')]\n"
     ]
    }
   ],
   "source": [
    "print(test.dtypes.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['isFraud'].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training, validation, and testing datasets\n",
    "\n",
    "Note that we have imbalanced datasets. As seen in Step 3, 96.76% transactions are non-fraud, compared to only 3.24% are fraud. We will apply some resampling techniques to balance the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT', 'TransactionID'], axis=1)\n",
    "y = df.sort_values('TransactionDT')['isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn import preprocessing\n",
    "#scaler = preprocessing.StandardScaler().fit(X)\n",
    "#X_scaled = scaler.transform(X)\n",
    "#y = y.ravel() # get 1-dim flattened array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 60% train, 20% validate, 20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "X_train, x_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size = 0.25, train_size =0.75, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test2 = test.sort_values('TransactionDT').drop(['TransactionDT', 'TransactionID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (411937, 67)\n",
      "Shape of y: (411937,)\n",
      "Number transactions X_train:  (247161, 67)\n",
      "Number transactions y_train:  (247161,)\n",
      "Number transactions X_cv:  (82388, 67)\n",
      "Number transactions y_cv:  (82388,)\n",
      "Number transactions X_test:  (82388, 67)\n",
      "Number transactions y_test:  (82388,)\n",
      "Number transactions X_test2:  (82388,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X: {}'.format(X.shape))\n",
    "print('Shape of y: {}'.format(y.shape))\n",
    "print(\"Number transactions X_train: \", X_train.shape)\n",
    "print(\"Number transactions y_train: \", y_train.shape)\n",
    "print(\"Number transactions X_cv: \", x_cv.shape)\n",
    "print(\"Number transactions y_cv: \", y_cv.shape)\n",
    "print(\"Number transactions X_test: \", X_test.shape)\n",
    "print(\"Number transactions y_test: \", y_test.shape)\n",
    "print(\"Number transactions X_test2: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling: SMOTE\n",
    "\n",
    "One approach to deal with imbalanced datasets is to oversample the minority class. A widely used approach is Synthetic Minority Oversampling Technique (SMOTE) for the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=123)\n",
    "X_train_sm, y_train_sm = sm.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (478516, 67)\n",
      "Shape of y: (478516,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X: {}'.format(X_train_sm.shape))\n",
    "print('Shape of y: {}'.format(y_train_sm.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
