{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 Preprocessing, Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Introduction**\n",
    "\n",
    "IEEE Computational Intelligence Society (IEEE-CIS) works across a variety of AI and machine learning areas, including deep neural networks, fuzzy systems, evolutionary computation, and swarm intelligence. Today they’re partnering with Vesta, the world’s leading payment service company, seeking the best solutions for the fraud detection industry. The fraud prevention system used by Vesta is actually saving consumers millions of dollars per year. Researchers from the IEEE-CIS want to improve fraud detection accuracy but also the customer experiences.\n",
    "\n",
    "**Data Source**\n",
    "\n",
    "The data comes from Vesta’s real-world e-commerce transactions and contains a wide range of features from device type to product features, available in Kaggle competition (https://www.kaggle.com/c/ieee-fraud-detection/data). Only train_identity and train_transaction datasets will be used for this project.\n",
    "\n",
    "**The Data Science Method**  \n",
    "\n",
    "1.   Problem Identification \n",
    "\n",
    "2.   Data Wrangling \n",
    " \n",
    "3.   Exploratory Data Analysis\n",
    "\n",
    "4.   **Pre-processing and Training Data Development**\n",
    "    - Create new features\n",
    "    - Standardize numeric features\n",
    "    - Split into testing and training datasets\n",
    "    - Resampling training dataset\n",
    "\n",
    "5.   Modeling \n",
    "\n",
    "6.   Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   isFraud  TransactionDT  TransactionAmt ProductCD  card1  card2  card3  \\\n",
      "0        0          86401            29.0         W   2755  404.0  150.0   \n",
      "1        0          86469            59.0         W   4663  490.0  150.0   \n",
      "2        0          86499            50.0         W  18132  567.0  150.0   \n",
      "3        0          86510            49.0         W   5937  555.0  150.0   \n",
      "4        0          86522           159.0         W  12308  360.0  150.0   \n",
      "\n",
      "        card4  card5   card6  ...  V281  V282 V283  V284  V286  V291  V297  \\\n",
      "0  mastercard  102.0  credit  ...   0.0   1.0  1.0   0.0   0.0   1.0   0.0   \n",
      "1        visa  166.0   debit  ...   0.0   1.0  1.0   0.0   0.0   1.0   0.0   \n",
      "2  mastercard  117.0   debit  ...   0.0   0.0  0.0   0.0   0.0   1.0   0.0   \n",
      "3        visa  226.0   debit  ...   0.0   1.0  1.0   0.0   0.0   1.0   0.0   \n",
      "4        visa  166.0   debit  ...   0.0   1.0  1.0   0.0   0.0   1.0   0.0   \n",
      "\n",
      "   V299 V305  V311  \n",
      "0   0.0  1.0   0.0  \n",
      "1   0.0  1.0   0.0  \n",
      "2   0.0  1.0   0.0  \n",
      "3   0.0  1.0   0.0  \n",
      "4   0.0  1.0   0.0  \n",
      "\n",
      "[5 rows x 59 columns]\n"
     ]
    }
   ],
   "source": [
    "path=\".../data\"\n",
    "os.chdir(path) \n",
    "\n",
    "df = pd.read_csv('step3_output.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['isFraud', 'TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'P_emaildomain', 'C3', 'D1', 'D4', 'D10', 'D15', 'M6', 'V25', 'V26', 'V46', 'V47', 'V55', 'V56', 'V61', 'V62', 'V66', 'V67', 'V77', 'V78', 'V82', 'V83', 'V98', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V114', 'V115', 'V116', 'V118', 'V120', 'V121', 'V122', 'V124', 'V281', 'V282', 'V283', 'V284', 'V286', 'V291', 'V297', 'V299', 'V305', 'V311']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>M6</th>\n",
       "      <td>0.182761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>addr2</th>\n",
       "      <td>0.139652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>addr1</th>\n",
       "      <td>0.139652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card2</th>\n",
       "      <td>0.017010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card5</th>\n",
       "      <td>0.007059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card3</th>\n",
       "      <td>0.003258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card4</th>\n",
       "      <td>0.003258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card6</th>\n",
       "      <td>0.003258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_emaildomain</th>\n",
       "      <td>0.003107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                percent\n",
       "M6             0.182761\n",
       "addr2          0.139652\n",
       "addr1          0.139652\n",
       "card2          0.017010\n",
       "card5          0.007059\n",
       "card3          0.003258\n",
       "card4          0.003258\n",
       "card6          0.003258\n",
       "P_emaildomain  0.003107"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nas = pd.DataFrame(df.isnull().sum().sort_values(ascending=False)/len(df),columns = ['percent'])\n",
    "pos = nas['percent'] > 0\n",
    "nas[pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Feature Engineering\n",
    "- Create new features: \n",
    "    - TransactionAmt_log - log of transaction amount\n",
    "    - Transaction_day - day of the week in which a transaction happened\n",
    "    - Transaction_hour - hour of the day in which a transaction happened\n",
    "- Card features: frequency encoding\n",
    "- P_emaildomain:\n",
    "    - Fill NAs with \"email_not_provided\"\n",
    "    - Split the email domain\n",
    "- Encode objects\n",
    "- V features: normalize against themselves\n",
    "- Fill NAs with -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New features\n",
    "df['TransactionAmt_log'] = np.log(df['TransactionAmt'])\n",
    "df['Transaction_day'] = np.floor((df['TransactionDT'] / (3600 * 24) - 1) % 7)\n",
    "df['Transaction_hour'] = np.floor(df['TransactionDT'] / 3600) % 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Card features encoding\n",
    "for col in ['card1', 'card2', 'card3', 'card4', 'card5', 'card6']:\n",
    "    freq = df[col].value_counts(dropna=False).to_dict()\n",
    "    df[col+'_freq'] = df[col].map(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# email feature\n",
    "df['P_emaildomain'] = df['P_emaildomain'].fillna('email_not_provided')\n",
    "df['P_prefix'] = df['P_emaildomain'].apply(lambda x: x.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objects\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "for col in df.drop('isFraud', axis=1).columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(df[col].astype(str).values))\n",
    "        df[col] = le.transform(list(df[col].astype(str).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V features - normalization\n",
    "V = df[['V25', 'V26', 'V46', 'V47', 'V55', 'V56', 'V61', 'V62', 'V66', 'V67', 'V77', 'V78', 'V82', 'V83', 'V98', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V114', 'V115', 'V116', 'V118', 'V120', 'V121', 'V122', 'V124', 'V281', 'V282', 'V283', 'V284', 'V286', 'V291', 'V297', 'V299', 'V305', 'V311']]\n",
    "for v in V:\n",
    "    df[v] = (df[v] - df[v].mean()) / df[v].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>addr1</th>\n",
       "      <td>0.139652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>addr2</th>\n",
       "      <td>0.139652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card2</th>\n",
       "      <td>0.017010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card5</th>\n",
       "      <td>0.007059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card3</th>\n",
       "      <td>0.003258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        percent\n",
       "addr1  0.139652\n",
       "addr2  0.139652\n",
       "card2  0.017010\n",
       "card5  0.007059\n",
       "card3  0.003258"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nas = pd.DataFrame(df.isnull().sum().sort_values(ascending=False)/len(df),columns = ['percent'])\n",
    "pos = nas['percent'] > 0\n",
    "nas[pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['addr1', 'addr2', 'card2', 'card5', 'card3']:\n",
    "    df[col].fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['isFraud'].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training and testing datasets\n",
    "\n",
    "Note that we have imbalanced datasets. As seen in Step 3, 96.76% transactions are non-fraud, compared to only 3.24% are fraud. We will apply some resampling techniques to balance the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT'], axis=1)\n",
    "y = df.sort_values('TransactionDT')['isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn import preprocessing\n",
    "#scaler = preprocessing.StandardScaler().fit(X)\n",
    "#X_scaled = scaler.transform(X)\n",
    "#y = y.ravel() # get 1-dim flattened array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (411937, 67)\n",
      "Shape of y: (411937,)\n",
      "Number transactions X_train:  (308952, 67)\n",
      "Number transactions y_train:  (308952,)\n",
      "Number transactions X_test:  (102985, 67)\n",
      "Number transactions y_test:  (102985,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X: {}'.format(X.shape))\n",
    "print('Shape of y: {}'.format(y.shape))\n",
    "print(\"Number transactions X_train: \", X_train.shape)\n",
    "print(\"Number transactions y_train: \", y_train.shape)\n",
    "print(\"Number transactions X_test: \", X_test.shape)\n",
    "print(\"Number transactions y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling: SMOTE\n",
    "\n",
    "One approach to deal with imbalanced datasets is to oversample the minority class. A widely used approach is Synthetic Minority Oversampling Technique (SMOTE) for the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=123)\n",
    "X_train_sm, y_train_sm = sm.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (598006, 67)\n",
      "Shape of y: (598006,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X: {}'.format(X_train_sm.shape))\n",
    "print('Shape of y: {}'.format(y_train_sm.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
