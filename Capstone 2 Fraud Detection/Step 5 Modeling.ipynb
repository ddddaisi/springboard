{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 Modeling\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "IEEE Computational Intelligence Society (IEEE-CIS) works across a variety of AI and machine learning areas, including deep neural networks, fuzzy systems, evolutionary computation, and swarm intelligence. Today they’re partnering with Vesta, the world’s leading payment service company, seeking the best solutions for the fraud detection industry. The fraud prevention system used by Vesta is actually saving consumers millions of dollars per year. Researchers from the IEEE-CIS want to improve fraud detection accuracy but also the customer experiences.\n",
    "\n",
    "**Data Source**\n",
    "\n",
    "The data comes from Vesta’s real-world e-commerce transactions and contains a wide range of features from device type to product features, available in Kaggle competition (https://www.kaggle.com/c/ieee-fraud-detection/data). Only train_identity and train_transaction datasets will be used for this project.\n",
    "\n",
    "**The Data Science Method**  \n",
    "\n",
    "1.   Problem Identification \n",
    "\n",
    "2.   Data Wrangling \n",
    " \n",
    "3.   Exploratory Data Analysis\n",
    "\n",
    "4.   Pre-processing and Training Data Development\n",
    "\n",
    "5.   **Modeling**\n",
    "    - Logistic Regression\n",
    "    - Decision Trees\n",
    "    - Random Forest - Ensemble Method\n",
    "    - Gradient Boosting/XGBoost\n",
    "\n",
    "6.   Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   isFraud  TransactionDT  TransactionAmt ProductCD  card1  card2  card3  \\\n",
      "0        0          86401            29.0         W   2755  404.0  150.0   \n",
      "1        0          86469            59.0         W   4663  490.0  150.0   \n",
      "2        0          86499            50.0         W  18132  567.0  150.0   \n",
      "3        0          86510            49.0         W   5937  555.0  150.0   \n",
      "4        0          86522           159.0         W  12308  360.0  150.0   \n",
      "\n",
      "        card4  card5   card6  ...  V281  V282 V283  V284  V286  V291  V297  \\\n",
      "0  mastercard  102.0  credit  ...   0.0   1.0  1.0   0.0   0.0   1.0   0.0   \n",
      "1        visa  166.0   debit  ...   0.0   1.0  1.0   0.0   0.0   1.0   0.0   \n",
      "2  mastercard  117.0   debit  ...   0.0   0.0  0.0   0.0   0.0   1.0   0.0   \n",
      "3        visa  226.0   debit  ...   0.0   1.0  1.0   0.0   0.0   1.0   0.0   \n",
      "4        visa  166.0   debit  ...   0.0   1.0  1.0   0.0   0.0   1.0   0.0   \n",
      "\n",
      "   V299 V305  V311  \n",
      "0   0.0  1.0   0.0  \n",
      "1   0.0  1.0   0.0  \n",
      "2   0.0  1.0   0.0  \n",
      "3   0.0  1.0   0.0  \n",
      "4   0.0  1.0   0.0  \n",
      "\n",
      "[5 rows x 59 columns]\n"
     ]
    }
   ],
   "source": [
    "path=\"/Users/huluhulu/Desktop/Capstone 2 Fraud Detection/data\"\n",
    "os.chdir(path) \n",
    "\n",
    "df = pd.read_csv('step3_output.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Feature Engineering\n",
    "See Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New feature - log of transaction amount\n",
    "df['TransactionAmt_log'] = np.log(df['TransactionAmt'])\n",
    "# New feature - day of the week when a transaction happened\n",
    "df['Transaction_day'] = np.floor((df['TransactionDT'] / (3600 * 24) - 1) % 7)\n",
    "# New feature - hour of the day when a transaction happened\n",
    "df['Transaction_hour'] = np.floor(df['TransactionDT'] / 3600) % 24\n",
    "\n",
    "# Card features encoding\n",
    "for col in ['card1', 'card2', 'card3', 'card4', 'card5', 'card6']:\n",
    "    freq = df[col].value_counts(dropna=False).to_dict()\n",
    "    df[col+'_freq'] = df[col].map(freq)\n",
    "\n",
    "# email feature\n",
    "df['P_emaildomain'] = df['P_emaildomain'].fillna('email_not_provided')\n",
    "df['P_prefix'] = df['P_emaildomain'].apply(lambda x: x.split('.')[0])\n",
    "\n",
    "# objects\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "for col in df.drop('isFraud', axis=1).columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(df[col].astype(str).values))\n",
    "        df[col] = le.transform(list(df[col].astype(str).values))\n",
    "        \n",
    "# V features - normalization\n",
    "V = df[['V25', 'V26', 'V46', 'V47', 'V55', 'V56', 'V61', 'V62', 'V66', 'V67', 'V77', 'V78', 'V82', 'V83', 'V98', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V114', 'V115', 'V116', 'V118', 'V120', 'V121', 'V122', 'V124', 'V281', 'V282', 'V283', 'V284', 'V286', 'V291', 'V297', 'V299', 'V305', 'V311']]\n",
    "for v in V:\n",
    "    df[v] = (df[v] - df[v].mean()) / df[v].std()\n",
    "\n",
    "# Fill NAs\n",
    "for col in ['addr1', 'addr2', 'card2', 'card5', 'card3']:\n",
    "    df[col].fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT'], axis=1)\n",
    "y = df.sort_values('TransactionDT')['isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)\n",
    "# X_train, x_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size = 0.25, train_size =0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling - SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=123)\n",
    "X_train_sm, y_train_sm = sm.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "We will choose the best models based on macro-averaged f1 score\n",
    "- **Recall** = TP/(TP+FN)\n",
    "    - fraud correctly detected / (fraud correctly detected + transaction identified as nonfraud that is actually fraud)\n",
    "    - as recall increases, precision decreases\n",
    "- **Precision** = TP/(TP+FP)\n",
    "    - fraud correctly detected / (fraud correctly detected + nonfraud transaction incorrectly idenfitied as fraud)\n",
    "- **F1 score** = 2 * (recall * precision)/(recall + precision)\n",
    "    - harmonic mean of recall and precision\n",
    "- **Balanced Accuracy**: the average of recall obtained on each class, used instead of accuracy to deal with imbalanced datasets\n",
    "- **Receiver Operating Characteristic (ROC)**:\n",
    "    - plots TPR vs FPR as a function of model's threshold for classifying positive\n",
    "    - TPR = TP/(TP+FN)\n",
    "    - FPR = FP/(FP+TN)\n",
    "- **Area Under the Curve (AUC)**:\n",
    "    - metric to calculate overal performance based on the area under the ROC curve\n",
    "- Averages\n",
    "    - micro-averaged: all samples equally contributed to the average; weight towards the majority class\n",
    "    - macro-averaged: all classes equally contributed; treats examples from minority classes as being more important than those from majority classes\n",
    "    - weighted: each class' contribution to the average is weighted by its size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [] \n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "auc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = clf.feature_importances_\n",
    "# make importances relative to max importance\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())[:30]\n",
    "sorted_idx = np.argsort(feature_importance)[:30]\n",
    "\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "print(pos.size)\n",
    "sorted_idx.size\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, X.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "- a form of binary regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, balanced_accuracy_score, roc_auc_score\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression, after oversampling\n",
      "Balanced accuracy score: 0.6794\n",
      "Precision score 0.5291\n",
      "Recall score 0.4901\n",
      "F1 score 0.4901\n",
      "AUC ROC score 0.6794\n"
     ]
    }
   ],
   "source": [
    "lr1 = LogisticRegression()\n",
    "lr1.fit(X_train_sm, y_train_sm)\n",
    "y_pred1 = lr1.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression, after oversampling\")\n",
    "print(\"Balanced accuracy score: {:.4f}\".format(balanced_accuracy_score(y_test, y_pred1)))\n",
    "print(\"Precision score {:.4f}\".format(precision_score(y_test, y_pred1, average='macro')))\n",
    "print(\"Recall score {:.4f}\".format(f1_score(y_test, y_pred1, average='macro')))\n",
    "print(\"F1 score {:.4f}\".format(f1_score(y_test, y_pred1, average='macro')))\n",
    "print(\"AUC ROC score {:.4f}\".format(roc_auc_score(y_test, y_pred1, average='macro')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we choose the macro parameter to emphasize the minority class (fraud transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set: {'C': 0.05}\n",
      "CPU times: user 3min 35s, sys: 13.8 s, total: 3min 49s\n",
      "Wall time: 2min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'C': [0.05, 0.1, 0.5, 1, 5, 10]}\n",
    "\n",
    "clf1 = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "clf1.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters set found on development set: {}\".format(clf1.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before oversampling with C=0.1, the f1 is 0.499994978911428\n"
     ]
    }
   ],
   "source": [
    "lr2 = LogisticRegression(C = 0.05)\n",
    "lr2.fit(X_train, y_train)\n",
    "y_pred2 = lr2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression, no oversampling\n",
      "Balanced accuracy score: 0.5000\n",
      "Precision score 0.4835\n",
      "Recall score 0.5000\n",
      "F1 score 0.4916\n",
      "AUC ROC score 0.5000\n"
     ]
    }
   ],
   "source": [
    "# model evaluation\n",
    "print(\"Logistic Regression, no oversampling\")\n",
    "print(\"Balanced accuracy score: {:.4f}\".format(balanced_accuracy_score(y_test, y_pred2)))\n",
    "print(\"Precision score {:.4f}\".format(precision_score(y_test, y_pred2, average='macro')))\n",
    "print(\"Recall score {:.4f}\".format(recall_score(y_test, y_pred2, average='macro')))\n",
    "print(\"F1 score {:.4f}\".format(f1_score(y_test, y_pred2, average='macro')))\n",
    "print(\"AUC ROC score {:.4f}\".format(roc_auc_score(y_test, y_pred2, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.append(\"Logistic Regression\")\n",
    "precision.append(precision_score(y_test, y_pred2, average='macro'))\n",
    "recall.append(recall_score(y_test, y_pred2, average='macro'))\n",
    "f1.append(f1_score(y_test, y_pred2, average='macro'))\n",
    "auc.append(roc_auc_score(y_test, y_pred2, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree\n",
    "- Entropy model\n",
    "    - It is a measure of uncertainty in which category the data-points fall into at a given point in the tree. In other words, given a probability distribution, the information required to predict the event is the distribution's entropy. \n",
    "    - Information gain of a specific feature with a threshold is the difference in entropy that exists before and after splitting on that feature.\n",
    "    - We want to minimize entropy and maximize informational gain\n",
    "    - Entropy outperforms gini if the proability distribution is exponential or Laplace\n",
    "- Gini impurity model\n",
    "    - Similar to entropy, a measure of how well the threshold splits the data. It's the expected error if randomly choose a sample and predict the class of the entire node based on it. The higher the gini impurity, the lower the error.\n",
    "    - It takes slightly more computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.48 s, sys: 166 ms, total: 4.65 s\n",
      "Wall time: 4.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "entro = tree.DecisionTreeClassifier(criterion=\"entropy\", random_state = 123, max_depth = 10)\n",
    "entro.fit(X_train, y_train)\n",
    "y_pred3 = entro.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Entropy Model, Max Depth 10\n",
      "Balanced accuracy score: 0.6444\n",
      "Precision score 0.8589\n",
      "Recall score 0.6444\n",
      "F1 score 0.7028\n",
      "ROC-AUC score 0.6444\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision Tree Entropy Model, Max Depth 10\")\n",
    "print(\"Balanced accuracy score: {:.4f}\".format(balanced_accuracy_score(y_test, y_pred3)))\n",
    "print(\"Precision score {:.4f}\".format(precision_score(y_test, y_pred3, average='macro')))\n",
    "print(\"Recall score {:.4f}\".format(recall_score(y_test, y_pred3, average='macro')))\n",
    "print(\"F1 score {:.4f}\".format(f1_score(y_test, y_pred3, average='macro')))\n",
    "print(\"ROC-AUC score {:.4f}\".format(roc_auc_score(y_test, y_pred3, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Entropy Model, Max Depth 8\n",
      "Balanced accuracy score: 0.6187\n",
      "Precision score 0.8380\n",
      "Recall score 0.6187\n",
      "F1 score 0.6720\n",
      "ROC-AUC score 0.6187\n",
      "CPU times: user 3.5 s, sys: 146 ms, total: 3.64 s\n",
      "Wall time: 3.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "entro2 = tree.DecisionTreeClassifier(criterion=\"entropy\", random_state = 123, max_depth = 8)\n",
    "entro2.fit(X_train, y_train)\n",
    "y_pred_3 = entro2.predict(X_test)\n",
    "print(\"Decision Tree Entropy Model, Max Depth 8\")\n",
    "print(\"Balanced accuracy score: {:.4f}\".format(balanced_accuracy_score(y_test, y_pred_3)))\n",
    "print(\"Precision score {:.4f}\".format(precision_score(y_test, y_pred_3, average='macro')))\n",
    "print(\"Recall score {:.4f}\".format(recall_score(y_test, y_pred_3, average='macro')))\n",
    "print(\"F1 score {:.4f}\".format(f1_score(y_test, y_pred_3, average='macro')))\n",
    "print(\"ROC-AUC score {:.4f}\".format(roc_auc_score(y_test, y_pred_3, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Gini Impurity Model, Max Depth 10\n",
      "Balanced accuracy score: 0.6250\n",
      "Precision score 0.8661\n",
      "Recall score 0.6250\n",
      "F1 score 0.6826\n",
      "ROC-AUC score 0.6250\n",
      "CPU times: user 4.54 s, sys: 154 ms, total: 4.69 s\n",
      "Wall time: 4.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gini = tree.DecisionTreeClassifier(criterion=\"gini\", random_state = 123, max_depth = 10)\n",
    "gini.fit(X_train, y_train)\n",
    "y_pred4 = gini.predict(X_test)\n",
    "\n",
    "print(\"Decision Tree Gini Impurity Model, Max Depth 10\")\n",
    "print(\"Balanced accuracy score: {:.4f}\".format(balanced_accuracy_score(y_test, y_pred4)))\n",
    "print(\"Precision score {:.4f}\".format(precision_score(y_test, y_pred4, average='macro')))\n",
    "print(\"Recall score {:.4f}\".format(recall_score(y_test, y_pred4, average='macro')))\n",
    "print(\"F1 score {:.4f}\".format(f1_score(y_test, y_pred4, average='macro')))\n",
    "print(\"ROC-AUC score {:.4f}\".format(roc_auc_score(y_test, y_pred4, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Gini Impurity Model, Max Depth 8\n",
      "Balanced accuracy score: 0.6029\n",
      "Precision score 0.8691\n",
      "Recall score 0.6029\n",
      "F1 score 0.6562\n",
      "ROC-AUC score 0.6029\n",
      "CPU times: user 3.74 s, sys: 179 ms, total: 3.92 s\n",
      "Wall time: 4.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gini = tree.DecisionTreeClassifier(criterion=\"gini\", random_state = 123, max_depth = 8)\n",
    "gini.fit(X_train, y_train)\n",
    "y_pred_4 = gini.predict(X_test)\n",
    "\n",
    "print(\"Decision Tree Gini Impurity Model, Max Depth 8\")\n",
    "print(\"Balanced accuracy score: {:.4f}\".format(balanced_accuracy_score(y_test, y_pred_4)))\n",
    "print(\"Precision score {:.4f}\".format(precision_score(y_test, y_pred_4, average='macro')))\n",
    "print(\"Recall score {:.4f}\".format(recall_score(y_test, y_pred_4, average='macro')))\n",
    "print(\"F1 score {:.4f}\".format(f1_score(y_test, y_pred_4, average='macro')))\n",
    "print(\"ROC-AUC score {:.4f}\".format(roc_auc_score(y_test, y_pred_4, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.append(\"Decision Tree Entropy, Max Depth 10\")\n",
    "precision.append(precision_score(y_test, y_pred3, average='macro'))\n",
    "recall.append(recall_score(y_test, y_pred3, average='macro'))\n",
    "f1.append(f1_score(y_test, y_pred3, average='macro'))\n",
    "auc.append(roc_auc_score(y_test, y_pred3, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest - Ensemble method\n",
    "\n",
    "It builds upon the idea of bagging - bootstrap aggregating. Bagging samples with replacement from the dataset: from some number of trees and predetermined depth, it selects a random subset of the data (by convention roughly 2/3 with replacement) and trains a decision tree on the subset of features. It learns a classifier for each bootstrap sample and average the results of classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Random Forest\n",
    "#%%time\n",
    "#param_grid = { \n",
    "#    'n_estimators': [200, 500],\n",
    "#    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "#    'max_depth' : [4,5,6,7,8],\n",
    "#    'criterion' :['gini', 'entropy']\n",
    "#}\n",
    "#rfc=RandomForestClassifier(random_state=123)\n",
    "#rfc_cv = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
    "#rfc_cv.fit(X_train, y_train)\n",
    "#print(\"Best Score:\" + str(rfc_cv.best_score_))\n",
    "#print(\"Best Parameters: \" + str(rfc_cv.best_params_))\n",
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 46s, sys: 2.65 s, total: 3min 48s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state = 123, n_jobs=-1)\n",
    "rfc.fit(X_train, y_train)\n",
    "y_pred5 = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Balanced accuracy score: 0.7119\n",
      "Precision score 0.9457\n",
      "Recall score 0.7119\n",
      "F1 score 0.7847\n",
      "ROC-AUC score 0.6250\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest\")\n",
    "print(\"Balanced accuracy score: {:.4f}\".format(balanced_accuracy_score(y_test, y_pred5)))\n",
    "print(\"Precision score {:.4f}\".format(precision_score(y_test, y_pred5, average='macro')))\n",
    "print(\"Recall score {:.4f}\".format(recall_score(y_test, y_pred5, average='macro')))\n",
    "print(\"F1 score {:.4f}\".format(f1_score(y_test, y_pred5, average='macro')))\n",
    "print(\"ROC-AUC score {:.4f}\".format(roc_auc_score(y_test, y_pred4, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.append(\"Random Forest\")\n",
    "precision.append(precision_score(y_test, y_pred5, average='macro'))\n",
    "recall.append(recall_score(y_test, y_pred5, average='macro'))\n",
    "f1.append(f1_score(y_test, y_pred5, average='macro'))\n",
    "auc.append(roc_auc_score(y_test, y_pred5, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### KNN - takes > 5 mins\n",
    "#test_scores = []\n",
    "#train_scores = []\n",
    "#\n",
    "#for i in range(1,10):\n",
    "#\n",
    "#    knn = KNeighborsClassifier(i)\n",
    "#    knn.fit(X_train,y_train)\n",
    "#    \n",
    "#    train_scores.append(knn.score(X_train,y_train))\n",
    "#    test_scores.append(knn.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting and XGBoost\n",
    "\n",
    "Graident Boosting learns from residual errors directly rather than update the weights of data points. Training is done iteratively, relies on results of the tree before it and places higher weights on data that got wrong previously. \n",
    "- Cons: harder to tune but easier to overfit (as it focus on those got wrong)\n",
    "- Pros: more powerful and better performance\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) is an implementation of gradient boosted decision trees, uses advanced regularization (L1&L2) to improve generalization capabilities and therefore reduce overfitting. It delivers higher speed and performance compared to Gradient Boosting and can be distributed across clusters. XGBoost allows users to not only define custom optimization objectives and evaluation criteria but also run a cross-validation at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gb = GradientBoostingClassifier(n_estimators=100, learning_rate = 0.5, min_samples_split=500, min_samples_leaf=50, max_depth=10, max_features='sqrt', subsample=0.8, random_state = 123)\n",
    "#gb.fit(X_train, y_train)\n",
    "#y_pred6 = gb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Tuning\n",
    "#model = xgb.XGBClassifier()\n",
    "#param = {\"max_depth\": [5,10,20,30,40,50],\n",
    "#              \"min_child_weight\" : [1,3,5],\n",
    "#              \"n_estimators\": [200],\n",
    "#              \"learning_rate\": [0.05, 0.1, 0.15, 0.2],}\n",
    "#grid_search = GridSearchCV(model, param_grid=param, cv = 5, n_jobs=-1)\n",
    "#grid_search.fit(X_train, y_train)\n",
    "#grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "Balanced accuracy score: 0.7491\n",
      "Precision score 0.9561\n",
      "Recall score 0.7491\n",
      "F1 score 0.8203\n",
      "ROC-AUC score 0.7491\n",
      "CPU times: user 21min 47s, sys: 6.12 s, total: 21min 54s\n",
      "Wall time: 22min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model1 = xgb.XGBClassifier(max_depth=50, min_child_weight=1, n_estimators=200, gamma=0,\\\n",
    "                           subsample=0.8, colsample_bytree=0.8, n_jobs=-1, learning_rate=0.05)\n",
    "model1.fit(X_train, y_train)\n",
    "pred1 = model1.predict(X_test)\n",
    "print(\"XGBoost\")\n",
    "print(\"Balanced accuracy score: {:.4f}\".format(balanced_accuracy_score(y_test, pred1)))\n",
    "print(\"Precision score {:.4f}\".format(precision_score(y_test, pred1, average='macro')))\n",
    "print(\"Recall score {:.4f}\".format(recall_score(y_test, pred1, average='macro')))\n",
    "print(\"F1 score {:.4f}\".format(f1_score(y_test, pred1, average='macro')))\n",
    "print(\"ROC-AUC score {:.4f}\".format(roc_auc_score(y_test, pred1, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.append(\"XGBoost\")\n",
    "precision.append(precision_score(y_test, pred1, average='macro'))\n",
    "recall.append(recall_score(y_test, pred1, average='macro'))\n",
    "f1.append(f1_score(y_test, pred1, average='macro'))\n",
    "auc.append(roc_auc_score(y_test, pred1, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC ROC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.483468</td>\n",
       "      <td>0.499995</td>\n",
       "      <td>0.491593</td>\n",
       "      <td>0.499995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree Entropy, Max Depth 10</td>\n",
       "      <td>0.858860</td>\n",
       "      <td>0.644366</td>\n",
       "      <td>0.702755</td>\n",
       "      <td>0.644366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.945696</td>\n",
       "      <td>0.711915</td>\n",
       "      <td>0.784668</td>\n",
       "      <td>0.711915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.956077</td>\n",
       "      <td>0.749127</td>\n",
       "      <td>0.820298</td>\n",
       "      <td>0.749127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Precision    Recall        F1  \\\n",
       "0                  Logistic Regression   0.483468  0.499995  0.491593   \n",
       "1  Decision Tree Entropy, Max Depth 10   0.858860  0.644366  0.702755   \n",
       "2                        Random Forest   0.945696  0.711915  0.784668   \n",
       "3                              XGBoost   0.956077  0.749127  0.820298   \n",
       "\n",
       "    AUC ROC  \n",
       "0  0.499995  \n",
       "1  0.644366  \n",
       "2  0.711915  \n",
       "3  0.749127  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelCompare = pd.DataFrame({'Model':[models[0], models[1], models[2], models[3]], \n",
    "                             'Precision': [precision[0], precision[1], precision[2], precision[3]],\n",
    "                             'Recall': [recall[0], recall[1], recall[2], recall[3]],\n",
    "                             'F1': [f1[0], f1[1], f1[2], f1[3]],\n",
    "                             'AUC ROC': [auc[0], auc[1], auc[2], auc[3]] })\n",
    "modelCompare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
